{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f8c499e-ed42-483f-b1b8-0c7fdd44f534",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\aiproject\\chatbot_project\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# 사전 설치 라이브러리\n",
    "# pip install gradio, pip install sqlalchemy\n",
    "# pip install langchain-community, pip install wordcloud, pip install pymysql\n",
    "import os\n",
    "import gradio as gr\n",
    "import glob\n",
    "from sqlalchemy import create_engine\n",
    "from config import Config\n",
    "from document_handler import DocumentHandler\n",
    "from image_handler import ImageHandler\n",
    "from chatbot import Chatbot\n",
    "from feedback_manager import QueryFeedbackManager\n",
    "from query_generator import QueryGenerator\n",
    "from natural_language_generator import NaturalLanguageGenerator\n",
    "from database_interface import DatabaseInterface\n",
    "from typing import List, Tuple, Optional, Dict, Any\n",
    "from datetime import datetime\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c71ce688-3241-4113-9903-bddaea147f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\aiproject\\chatbot_project\\query_generator.py:41: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  self.llm = Ollama(model=\"gemma2\", temperature=0)\n",
      "c:\\Python310\\aiproject\\chatbot_project\\query_generator.py:42: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  self.chain = LLMChain(llm=self.llm, prompt=self.prompt)\n"
     ]
    }
   ],
   "source": [
    "# 컴포넌트 초기화\n",
    "db_interface = DatabaseInterface()\n",
    "query_generator = QueryGenerator()\n",
    "nl_generator = NaturalLanguageGenerator()\n",
    "feedback_manager = QueryFeedbackManager(Config.FEEDBACK_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bf79de7-71ef-4424-b952-1b2469c4caca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_temp_files():\n",
    "    \"\"\"temp 폴더 내 파일 목록을 반환하는 함수\"\"\"\n",
    "    if not os.path.exists(Config.TEMP_FOLDER):\n",
    "        os.makedirs(Config.TEMP_FOLDER)\n",
    "    return [os.path.basename(f) for f in glob.glob(os.path.join(Config.TEMP_FOLDER, \"*.txt\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9feae08b-4834-4a70-8c7a-1dce87f04089",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_question(question: str) -> Tuple[str, str, str]:\n",
    "        \"\"\"사용자 질문을 처리하고 결과를 반환합니다.\"\"\"\n",
    "        feedback_manager.last_question = question\n",
    "        \n",
    "        # 유사한 피드백 찾기\n",
    "        similar_feedback = feedback_manager.find_similar_feedback(question)\n",
    "        feedback_info = f\"이전 피드백: {similar_feedback['feedback']}\" if similar_feedback else \"\"\n",
    "        \n",
    "        # 쿼리 생성 및 실행\n",
    "        schema_info = db_interface.get_schema_info()\n",
    "        query = query_generator.generate_query(question, schema_info, feedback_info)\n",
    "        columns, results = db_interface.execute_query(query)\n",
    "        \n",
    "        # 결과 생성\n",
    "        if columns is None:\n",
    "            return \"쿼리 실행 실패\", \"오류가 발생했습니다.\", query\n",
    "        \n",
    "        result_text = \"\\n\".join([str(row) for row in results])\n",
    "        natural_explanation = nl_generator.generate_summary(columns, results)\n",
    "        \n",
    "        return result_text, natural_explanation, query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65372883-df94-4b5b-860e-8553f77d1eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_feedback(feedback: str) -> str:\n",
    "        \"\"\"피드백을 처리하고 저장합니다.\"\"\"\n",
    "        if not feedback_manager.last_question:\n",
    "            return \"피드백을 저장할 질문이 없습니다.\"\n",
    "        \n",
    "        feedback_data = feedback_manager.load_feedback()\n",
    "        feedback_data.append({\n",
    "            \"id\": len(feedback_data) + 1,\n",
    "            \"question\": feedback_manager.last_question,\n",
    "            \"feedback\": feedback,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        })\n",
    "        feedback_manager.save_feedback(feedback_data)\n",
    "        return \"피드백이 성공적으로 저장되었습니다. 다음 검색에 반영됩니다.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69247c86-219f-4ea8-bd46-2237d9f9f8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\aiproject\\chatbot_project\\document_handler.py:11: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  self.embeddings_model = HuggingFaceEmbeddings(\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n",
      "c:\\Python310\\aiproject\\chatbot_project\\chatbot.py:22: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  self.llm = ChatOllama(model=Config.GEMMA_MODEL, temperature=Config.TEMPERATURE)\n",
      "c:\\Python310\\aiproject\\chatbot_project\\chatbot.py:24: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  self.memory = ConversationBufferMemory(\n",
      "INFO:chromadb.telemetry.product.posthog:Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    }
   ],
   "source": [
    "document_handler = DocumentHandler()\n",
    "image_handler = ImageHandler()\n",
    "chatbot = Chatbot(document_handler, image_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d7a580f-eb8e-470f-afdf-946c4f5ec0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yj\\AppData\\Local\\Temp\\ipykernel_19048\\1506479090.py:7: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot_interface = gr.Chatbot(height=600)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "with gr.Blocks(theme=gr.themes.Soft(), css=\"footer {display: none !important}\") as iface:\n",
    "    gr.Markdown(\"# 정보 제공 AI 챗봇\")\n",
    "\n",
    "    with gr.Tabs():\n",
    "        # 채팅 탭\n",
    "        with gr.TabItem(\"채팅\"):\n",
    "            chatbot_interface = gr.Chatbot(height=600)\n",
    "            msg = gr.Textbox(label=\"질문을 입력하세요\", lines=1)\n",
    "            image_input = gr.Image(type=\"pil\", label=\"이미지 업로드 (선택사항)\")\n",
    "\n",
    "            with gr.Row():\n",
    "                submit_btn = gr.Button(\"전송\", variant=\"primary\")\n",
    "                clear_btn = gr.Button(\"대화 내용 지우기\")\n",
    "\n",
    "            gr.Examples(\n",
    "                examples=[\n",
    "                    [\"휴먼(주)의 주소는 어디인가요?\", None],\n",
    "                    [\"휴먼(주)가 보유한 솔루션은 무엇인가요?\", None],\n",
    "                    [\"휴먼(주)의 영업대표는 누구인가요?\", None]\n",
    "                ],\n",
    "                inputs=[msg, image_input]\n",
    "            )\n",
    "\n",
    "            msg.submit(chatbot.chat, [msg, chatbot_interface, image_input], \n",
    "                     [msg, chatbot_interface, image_input])\n",
    "            submit_btn.click(chatbot.chat, [msg, chatbot_interface, image_input], \n",
    "                           [msg, chatbot_interface, image_input])\n",
    "            clear_btn.click(lambda: (None, None), None, [chatbot_interface, image_input])\n",
    "\n",
    "        # 정보 추가 탭\n",
    "        with gr.TabItem(\"정보 추가\"):\n",
    "            new_info_input = gr.Textbox(label=\"새로운 정보 추가\", lines=3)\n",
    "            add_info_btn = gr.Button(\"정보 추가하기\")\n",
    "            info_status = gr.Textbox(label=\"상태 메시지\", interactive=False)\n",
    "\n",
    "            add_info_btn.click(document_handler.add_new_information, \n",
    "                             [new_info_input], \n",
    "                             [info_status])\n",
    "\n",
    "        # 파일 관리 탭\n",
    "        with gr.TabItem(\"파일 관리\"):\n",
    "            gr.Markdown(\"## temp 폴더에 파일 추가 및 질문\")\n",
    "\n",
    "            file_input = gr.File(label=\"Windows에서 파일 추가\")\n",
    "            add_file_btn = gr.Button(\"파일 추가하기\")\n",
    "            file_status = gr.Textbox(label=\"상태 메시지\", interactive=False)\n",
    "\n",
    "            file_list = gr.Dropdown(\n",
    "                label=\"temp 폴더의 파일 선택\",\n",
    "                choices=list_temp_files(),\n",
    "                multiselect=False,\n",
    "                interactive=True\n",
    "            )\n",
    "\n",
    "            def update_file_list(file):\n",
    "                status_message, _ = document_handler.add_new_file(file)\n",
    "                updated_files = list_temp_files()\n",
    "                return status_message, gr.update(choices=updated_files)\n",
    "\n",
    "            add_file_btn.click(\n",
    "                update_file_list,\n",
    "                inputs=[file_input],\n",
    "                outputs=[file_status, file_list]\n",
    "            )\n",
    "\n",
    "            question_input = gr.Textbox(label=\"질문 입력\", lines=2, \n",
    "                                      placeholder=\"질문을 입력하세요.\")\n",
    "            ask_btn = gr.Button(\"질문 전송\", variant=\"primary\")\n",
    "            answer_box = gr.Textbox(label=\"답변\", interactive=False)\n",
    "\n",
    "            def ask_from_selected_file(selected_file, question):\n",
    "                if not selected_file:\n",
    "                    return \"먼저 파일을 선택해주세요.\"\n",
    "                if not question.strip():\n",
    "                    return \"질문을 입력해주세요.\"\n",
    "\n",
    "                try:\n",
    "                    file_path = os.path.join(Config.TEMP_FOLDER, selected_file)\n",
    "                    if not os.path.exists(file_path):\n",
    "                        return f\"선택된 파일을 찾을 수 없습니다: {selected_file}\"\n",
    "\n",
    "                    custom_db = document_handler.initialize_db(file_path, use_main_only=False)\n",
    "                    temp_qa_chain = chatbot.initialize_qa_chain(custom_db)\n",
    "\n",
    "                    response = temp_qa_chain({\"question\": question, \"chat_history\": []})\n",
    "\n",
    "                    answer = response[\"answer\"]\n",
    "                    if 'source_documents' in response:\n",
    "                        sources = set([os.path.basename(doc.metadata.get('source', 'Unknown')) \n",
    "                                     for doc in response['source_documents']])\n",
    "                        source_info = f\"\\n\\n참고 파일: {', '.join(sources)}\" if sources else \"\"\n",
    "                        answer += source_info\n",
    "\n",
    "                    return answer\n",
    "\n",
    "                except Exception as e:\n",
    "                    return f\"오류가 발생했습니다: {str(e)}\"\n",
    "\n",
    "            ask_btn.click(\n",
    "                ask_from_selected_file,\n",
    "                inputs=[file_list, question_input],\n",
    "                outputs=answer_box\n",
    "            )\n",
    "\n",
    "        # 전체 내용 보기 탭\n",
    "        with gr.TabItem(\"전체 내용 보기\"):\n",
    "            view_content_btn = gr.Button(\"전체 내용 보기\")\n",
    "            content_display = gr.Textbox(label=\"전체 내용\", lines=10)\n",
    "\n",
    "            def view_all_content():\n",
    "                try:\n",
    "                    with open(Config.MAIN_DOC_PATH, 'r', encoding='utf-8') as f:\n",
    "                        content = \"=== 메인 문서 내용 ===\\n\" + f.read().strip() + \"\\n\\n\"\n",
    "\n",
    "                    if os.path.exists(Config.TEMP_FOLDER):\n",
    "                        for filename in os.listdir(Config.TEMP_FOLDER):\n",
    "                            if filename.endswith('.txt'):\n",
    "                                file_path = os.path.join(Config.TEMP_FOLDER, filename)\n",
    "                                try:\n",
    "                                    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                                        content += f\"=== {filename} ===\\n{f.read().strip()}\\n\\n\"\n",
    "                                except Exception as e:\n",
    "                                    content += f\"=== {filename} ===\\n(읽기 오류: {str(e)})\\n\\n\"\n",
    "                    else:\n",
    "                        content += \"temp 폴더가 존재하지 않습니다.\\n\\n\"\n",
    "\n",
    "                    return content\n",
    "                except Exception as e:\n",
    "                    return f\"전체 내용 보기 중 오류 발생: {str(e)}\"\n",
    "\n",
    "            view_content_btn.click(view_all_content, None, content_display)\n",
    "\n",
    "        # 단어시각화 탭\n",
    "        with gr.TabItem(\"단어시각화\"):\n",
    "            generate_btn = gr.Button(\"Word Cloud 생성\")\n",
    "            output_image = gr.Image()\n",
    "            generate_btn.click(\n",
    "                lambda: image_handler.generate_wordcloud(), \n",
    "                None, \n",
    "                output_image\n",
    "            )\n",
    "            \n",
    "        # DB연결 조회 탭\n",
    "        with gr.TabItem(\"DB연결 조회\"):\n",
    "            question_input = gr.Textbox(\n",
    "                label=\"질문 입력\",\n",
    "                placeholder=\"데이터베이스 관련 질문을 입력하세요.\",\n",
    "                lines=2\n",
    "            )\n",
    "            submit_button = gr.Button(\"질문 제출\")\n",
    "            query_area = gr.Textbox(label=\"생성된 쿼리\", lines=4, interactive=False)\n",
    "            result_area = gr.Textbox(label=\"쿼리 결과\", lines=4, interactive=False)\n",
    "            explanation_area = gr.Textbox(label=\"자연어 설명\", lines=6, interactive=False)\n",
    "\n",
    "            submit_button.click(\n",
    "                process_question,\n",
    "                inputs=question_input,\n",
    "                outputs=[result_area, explanation_area, query_area]\n",
    "            )\n",
    "\n",
    "        # 피드백 제공 탭\n",
    "        with gr.TabItem(\"피드백 제공\"):\n",
    "            feedback_input = gr.Textbox(label=\"피드백\", lines=4)\n",
    "            feedback_button = gr.Button(\"피드백 제출\")\n",
    "            feedback_output = gr.Textbox(label=\"상태\", interactive=False)\n",
    "\n",
    "            feedback_button.click(\n",
    "                handle_feedback,\n",
    "                inputs=feedback_input,\n",
    "                outputs=feedback_output\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13be1421-5aeb-436a-920e-e7b633cf0ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: HEAD http://127.0.0.1:7861/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rerunning server... use `close()` to stop if you need to change `launch()` parameters.\n",
      "----\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iface.launch(\n",
    "        server_port=7861,\n",
    "        server_name=\"127.0.0.1\",\n",
    "        inbrowser=False,  # 자체 브라우저 실행은 비활성화\n",
    "        show_error=True,  # 오류 표시 활성화\n",
    "        quiet=False,       # 로그 출력 활성화\n",
    "        prevent_thread_lock=True  # 스레드 블로킹 방지\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6aabc9b-9b17-41fe-a571-d9355cd48096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iface.close(7860)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5101f94e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18104"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Garbage Collection 실행\n",
    "# gc.collect()  # 메모리 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84b8839",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
