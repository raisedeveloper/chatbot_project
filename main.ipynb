{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f8c499e-ed42-483f-b1b8-0c7fdd44f534",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\aiproject\\chatbot_project\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# 사전 설치 라이브러리\n",
    "# pip install gradio, pip install sqlalchemy\n",
    "# pip install langchain-community, pip install wordcloud, pip install pymysql\n",
    "import os\n",
    "import gradio as gr\n",
    "import glob\n",
    "from sqlalchemy import create_engine\n",
    "from config import Config\n",
    "from document_handler import DocumentHandler\n",
    "from image_handler import ImageHandler\n",
    "from chatbot import Chatbot\n",
    "from feedback_manager import QueryFeedbackManager\n",
    "from query_generator import QueryGenerator\n",
    "from natural_language_generator import NaturalLanguageGenerator\n",
    "from database_interface import DatabaseInterface\n",
    "from typing import List, Tuple, Optional, Dict, Any\n",
    "from datetime import datetime\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c71ce688-3241-4113-9903-bddaea147f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\aiproject\\chatbot_project\\query_generator.py:41: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  self.llm = Ollama(model=\"gemma2\", temperature=0)\n",
      "c:\\Python310\\aiproject\\chatbot_project\\query_generator.py:42: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  self.chain = LLMChain(llm=self.llm, prompt=self.prompt)\n"
     ]
    }
   ],
   "source": [
    "# 컴포넌트 초기화\n",
    "db_interface = DatabaseInterface()\n",
    "query_generator = QueryGenerator()\n",
    "nl_generator = NaturalLanguageGenerator()\n",
    "feedback_manager = QueryFeedbackManager(Config.FEEDBACK_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bf79de7-71ef-4424-b952-1b2469c4caca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_temp_files():\n",
    "    \"\"\"temp 폴더 내 파일 목록을 반환하는 함수\"\"\"\n",
    "    if not os.path.exists(Config.TEMP_FOLDER):\n",
    "        os.makedirs(Config.TEMP_FOLDER)\n",
    "    return [os.path.basename(f) for f in glob.glob(os.path.join(Config.TEMP_FOLDER, \"*.txt\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9feae08b-4834-4a70-8c7a-1dce87f04089",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_question(question: str) -> Tuple[str, str, str]:\n",
    "        \"\"\"사용자 질문을 처리하고 결과를 반환합니다.\"\"\"\n",
    "        feedback_manager.last_question = question\n",
    "        \n",
    "        # 유사한 피드백 찾기\n",
    "        similar_feedback = feedback_manager.find_similar_feedback(question)\n",
    "        feedback_info = f\"이전 피드백: {similar_feedback['feedback']}\" if similar_feedback else \"\"\n",
    "        \n",
    "        # 쿼리 생성 및 실행\n",
    "        schema_info = db_interface.get_schema_info()\n",
    "        query = query_generator.generate_query(question, schema_info, feedback_info)\n",
    "        columns, results = db_interface.execute_query(query)\n",
    "        \n",
    "        # 결과 생성\n",
    "        if columns is None:\n",
    "            return \"쿼리 실행 실패\", \"오류가 발생했습니다.\", query\n",
    "        \n",
    "        result_text = \"\\n\".join([str(row) for row in results])\n",
    "        natural_explanation = nl_generator.generate_summary(columns, results)\n",
    "        \n",
    "        return result_text, natural_explanation, query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65372883-df94-4b5b-860e-8553f77d1eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_feedback(feedback: str) -> str:\n",
    "        \"\"\"피드백을 처리하고 저장합니다.\"\"\"\n",
    "        if not feedback_manager.last_question:\n",
    "            return \"피드백을 저장할 질문이 없습니다.\"\n",
    "        \n",
    "        feedback_data = feedback_manager.load_feedback()\n",
    "        feedback_data.append({\n",
    "            \"id\": len(feedback_data) + 1,\n",
    "            \"question\": feedback_manager.last_question,\n",
    "            \"feedback\": feedback,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        })\n",
    "        feedback_manager.save_feedback(feedback_data)\n",
    "        return \"피드백이 성공적으로 저장되었습니다. 다음 검색에 반영됩니다.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69247c86-219f-4ea8-bd46-2237d9f9f8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\aiproject\\chatbot_project\\document_handler.py:11: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  self.embeddings_model = HuggingFaceEmbeddings(\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n",
      "c:\\Python310\\aiproject\\chatbot_project\\chatbot.py:22: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  self.llm = ChatOllama(model=Config.GEMMA_MODEL, temperature=Config.TEMPERATURE)\n",
      "c:\\Python310\\aiproject\\chatbot_project\\chatbot.py:24: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  self.memory = ConversationBufferMemory(\n",
      "INFO:chromadb.telemetry.product.posthog:Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    }
   ],
   "source": [
    "document_handler = DocumentHandler()\n",
    "image_handler = ImageHandler()\n",
    "chatbot = Chatbot(document_handler, image_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d7a580f-eb8e-470f-afdf-946c4f5ec0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yj\\AppData\\Local\\Temp\\ipykernel_10848\\1506479090.py:7: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot_interface = gr.Chatbot(height=600)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "with gr.Blocks(theme=gr.themes.Soft(), css=\"footer {display: none !important}\") as iface:\n",
    "    gr.Markdown(\"# 정보 제공 AI 챗봇\")\n",
    "\n",
    "    with gr.Tabs():\n",
    "        # 채팅 탭\n",
    "        with gr.TabItem(\"채팅\"):\n",
    "            chatbot_interface = gr.Chatbot(height=600)\n",
    "            msg = gr.Textbox(label=\"질문을 입력하세요\", lines=1)\n",
    "            image_input = gr.Image(type=\"pil\", label=\"이미지 업로드 (선택사항)\")\n",
    "\n",
    "            with gr.Row():\n",
    "                submit_btn = gr.Button(\"전송\", variant=\"primary\")\n",
    "                clear_btn = gr.Button(\"대화 내용 지우기\")\n",
    "\n",
    "            gr.Examples(\n",
    "                examples=[\n",
    "                    [\"휴먼(주)의 주소는 어디인가요?\", None],\n",
    "                    [\"휴먼(주)가 보유한 솔루션은 무엇인가요?\", None],\n",
    "                    [\"휴먼(주)의 영업대표는 누구인가요?\", None]\n",
    "                ],\n",
    "                inputs=[msg, image_input]\n",
    "            )\n",
    "\n",
    "            msg.submit(chatbot.chat, [msg, chatbot_interface, image_input], \n",
    "                     [msg, chatbot_interface, image_input])\n",
    "            submit_btn.click(chatbot.chat, [msg, chatbot_interface, image_input], \n",
    "                           [msg, chatbot_interface, image_input])\n",
    "            clear_btn.click(lambda: (None, None), None, [chatbot_interface, image_input])\n",
    "\n",
    "        # 정보 추가 탭\n",
    "        with gr.TabItem(\"정보 추가\"):\n",
    "            new_info_input = gr.Textbox(label=\"새로운 정보 추가\", lines=3)\n",
    "            add_info_btn = gr.Button(\"정보 추가하기\")\n",
    "            info_status = gr.Textbox(label=\"상태 메시지\", interactive=False)\n",
    "\n",
    "            add_info_btn.click(document_handler.add_new_information, \n",
    "                             [new_info_input], \n",
    "                             [info_status])\n",
    "\n",
    "        # 파일 관리 탭\n",
    "        with gr.TabItem(\"파일 관리\"):\n",
    "            gr.Markdown(\"## temp 폴더에 파일 추가 및 질문\")\n",
    "\n",
    "            file_input = gr.File(label=\"Windows에서 파일 추가\")\n",
    "            add_file_btn = gr.Button(\"파일 추가하기\")\n",
    "            file_status = gr.Textbox(label=\"상태 메시지\", interactive=False)\n",
    "\n",
    "            file_list = gr.Dropdown(\n",
    "                label=\"temp 폴더의 파일 선택\",\n",
    "                choices=list_temp_files(),\n",
    "                multiselect=False,\n",
    "                interactive=True\n",
    "            )\n",
    "\n",
    "            def update_file_list(file):\n",
    "                status_message, _ = document_handler.add_new_file(file)\n",
    "                updated_files = list_temp_files()\n",
    "                return status_message, gr.update(choices=updated_files)\n",
    "\n",
    "            add_file_btn.click(\n",
    "                update_file_list,\n",
    "                inputs=[file_input],\n",
    "                outputs=[file_status, file_list]\n",
    "            )\n",
    "\n",
    "            question_input = gr.Textbox(label=\"질문 입력\", lines=2, \n",
    "                                      placeholder=\"질문을 입력하세요.\")\n",
    "            ask_btn = gr.Button(\"질문 전송\", variant=\"primary\")\n",
    "            answer_box = gr.Textbox(label=\"답변\", interactive=False)\n",
    "\n",
    "            def ask_from_selected_file(selected_file, question):\n",
    "                if not selected_file:\n",
    "                    return \"먼저 파일을 선택해주세요.\"\n",
    "                if not question.strip():\n",
    "                    return \"질문을 입력해주세요.\"\n",
    "\n",
    "                try:\n",
    "                    file_path = os.path.join(Config.TEMP_FOLDER, selected_file)\n",
    "                    if not os.path.exists(file_path):\n",
    "                        return f\"선택된 파일을 찾을 수 없습니다: {selected_file}\"\n",
    "\n",
    "                    custom_db = document_handler.initialize_db(file_path, use_main_only=False)\n",
    "                    temp_qa_chain = chatbot.initialize_qa_chain(custom_db)\n",
    "\n",
    "                    response = temp_qa_chain({\"question\": question, \"chat_history\": []})\n",
    "\n",
    "                    answer = response[\"answer\"]\n",
    "                    if 'source_documents' in response:\n",
    "                        sources = set([os.path.basename(doc.metadata.get('source', 'Unknown')) \n",
    "                                     for doc in response['source_documents']])\n",
    "                        source_info = f\"\\n\\n참고 파일: {', '.join(sources)}\" if sources else \"\"\n",
    "                        answer += source_info\n",
    "\n",
    "                    return answer\n",
    "\n",
    "                except Exception as e:\n",
    "                    return f\"오류가 발생했습니다: {str(e)}\"\n",
    "\n",
    "            ask_btn.click(\n",
    "                ask_from_selected_file,\n",
    "                inputs=[file_list, question_input],\n",
    "                outputs=answer_box\n",
    "            )\n",
    "\n",
    "        # 전체 내용 보기 탭\n",
    "        with gr.TabItem(\"전체 내용 보기\"):\n",
    "            view_content_btn = gr.Button(\"전체 내용 보기\")\n",
    "            content_display = gr.Textbox(label=\"전체 내용\", lines=10)\n",
    "\n",
    "            def view_all_content():\n",
    "                try:\n",
    "                    with open(Config.MAIN_DOC_PATH, 'r', encoding='utf-8') as f:\n",
    "                        content = \"=== 메인 문서 내용 ===\\n\" + f.read().strip() + \"\\n\\n\"\n",
    "\n",
    "                    if os.path.exists(Config.TEMP_FOLDER):\n",
    "                        for filename in os.listdir(Config.TEMP_FOLDER):\n",
    "                            if filename.endswith('.txt'):\n",
    "                                file_path = os.path.join(Config.TEMP_FOLDER, filename)\n",
    "                                try:\n",
    "                                    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                                        content += f\"=== {filename} ===\\n{f.read().strip()}\\n\\n\"\n",
    "                                except Exception as e:\n",
    "                                    content += f\"=== {filename} ===\\n(읽기 오류: {str(e)})\\n\\n\"\n",
    "                    else:\n",
    "                        content += \"temp 폴더가 존재하지 않습니다.\\n\\n\"\n",
    "\n",
    "                    return content\n",
    "                except Exception as e:\n",
    "                    return f\"전체 내용 보기 중 오류 발생: {str(e)}\"\n",
    "\n",
    "            view_content_btn.click(view_all_content, None, content_display)\n",
    "\n",
    "        # 단어시각화 탭\n",
    "        with gr.TabItem(\"단어시각화\"):\n",
    "            generate_btn = gr.Button(\"Word Cloud 생성\")\n",
    "            output_image = gr.Image()\n",
    "            generate_btn.click(\n",
    "                lambda: image_handler.generate_wordcloud(), \n",
    "                None, \n",
    "                output_image\n",
    "            )\n",
    "            \n",
    "        # DB연결 조회 탭\n",
    "        with gr.TabItem(\"DB연결 조회\"):\n",
    "            question_input = gr.Textbox(\n",
    "                label=\"질문 입력\",\n",
    "                placeholder=\"데이터베이스 관련 질문을 입력하세요.\",\n",
    "                lines=2\n",
    "            )\n",
    "            submit_button = gr.Button(\"질문 제출\")\n",
    "            query_area = gr.Textbox(label=\"생성된 쿼리\", lines=4, interactive=False)\n",
    "            result_area = gr.Textbox(label=\"쿼리 결과\", lines=4, interactive=False)\n",
    "            explanation_area = gr.Textbox(label=\"자연어 설명\", lines=6, interactive=False)\n",
    "\n",
    "            submit_button.click(\n",
    "                process_question,\n",
    "                inputs=question_input,\n",
    "                outputs=[result_area, explanation_area, query_area]\n",
    "            )\n",
    "\n",
    "        # 피드백 제공 탭\n",
    "        with gr.TabItem(\"피드백 제공\"):\n",
    "            feedback_input = gr.Textbox(label=\"피드백\", lines=4)\n",
    "            feedback_button = gr.Button(\"피드백 제출\")\n",
    "            feedback_output = gr.Textbox(label=\"상태\", interactive=False)\n",
    "\n",
    "            feedback_button.click(\n",
    "                handle_feedback,\n",
    "                inputs=feedback_input,\n",
    "                outputs=feedback_output\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13be1421-5aeb-436a-920e-e7b633cf0ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:7860/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: HEAD http://127.0.0.1:7860/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iface.launch(\n",
    "        server_port=7860,\n",
    "        server_name=\"127.0.0.1\",\n",
    "        inbrowser=False,  # 자체 브라우저 실행은 비활성화\n",
    "        show_error=True,  # 오류 표시 활성화\n",
    "        quiet=False,       # 로그 출력 활성화\n",
    "        prevent_thread_lock=True  # 스레드 블로킹 방지\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6aabc9b-9b17-41fe-a571-d9355cd48096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "iface.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5101f94e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16977"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\aiproject\\chatbot_project\\query_generator.py:46: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = self.chain.run(\n",
      "C:\\Users\\yj\\AppData\\Local\\Temp\\ipykernel_10848\\1506479090.py:85: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = temp_qa_chain({\"question\": question, \"chat_history\": []})\n",
      "WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 3 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    당신은 휴먼(주) 정보를 제공하는 AI 어시스턴트입니다. 모든 답변은 한국어로 답변해 주세요.\n",
      "\n",
      "    아래는 이전 대화 내용입니다:\n",
      "    \n",
      "\n",
      "    관련 문서 내용:\n",
      "    한국의 역사는 수천 년에 걸쳐 이어져 온 긴 여정 속에서 다양한 문화와 전통이 형성되고 발전해 왔습니다. 고조선에서 시작해 삼국 시대의 경쟁, 그리고 통일 신라와 고려를 거쳐 조선까지, 한반도는 많은 변화를 겪었습니다.\n",
      "\n",
      "고조선은 기원전 2333년 단군왕검에 의해 세워졌다고 전해집니다. 이는 한국 역사상 최초의 국가로, 한민족의 시원이라 할 수 있습니다. 이후 기원전 1세기경에는 한반도와 만주 일대에서 여러 소국이 성장하며 삼한 시대로 접어듭니다.\n",
      "\n",
      "4세기경, 고구려, 백제, 신라의 삼국이 한반도의 주요 세력으로 부상했습니다. 이 시기는 삼국이 각각 문화와 기술, 무력을 발전시키며 경쟁적으로 성장한 시기로, 한국 역사에서 중요한 전환점을 마련했습니다. 특히 고구려는 북방의 강대국으로 성장하여 중국과도 여러 차례 전쟁을 벌였습니다.\n",
      "\n",
      "7세기 말, 신라는 당나라와 연합하여 백제와 고구려를 차례로 정복하고, 한반도 최초의 통일 국가인 통일 신라를 건립합니다. 이 시기에 신라는 불교를 국교로 채택하며 문화와 예술이 크게 발전했습니다.\n",
      "\n",
      "그러나 10세기에 이르러 신라는 내부의 분열과 외부의 압력으로 쇠퇴하고, 이를 대체하여 고려가 성립됩니다. 고려 시대에는 과거제도의 도입과 더불어 청자 등 고려 고유의 문화가 꽃피었습니다.\n",
      "\n",
      "조선은 1392년 이성계에 의해 건국되어, 1910년까지 이어졌습니다. 조선 초기에는 세종대왕이 한글을 창제하여 백성들의 문해율을 높이는 등 문화적, 과학적 성취가 이루어졌습니다. 그러나 조선 후기에는 내부적으로 실학의 발전과 함께 사회적 변화가 모색되었으나, 외부로부터의 압력은 점차 커져만 갔습니다.\n",
      "\n",
      "19세기 말부터 20세기 초에 걸쳐 한국은 제국주의 열강의 침략을 받으며 많은 시련을 겪었습니다. 1910년, 한국은 일본에 의해 강제로 병합되어 35년간의 식민 지배를 받게 됩니다. 이 기간 동안 한국인들은 독립을 위한 다양한 운동을 전개했으며, 이는 1945년 일본의 패망으로 이어지는 독립으로 결실을 맺었습니다.\n",
      "\n",
      "해방 후 한반도는 남북으로 분단되어 각각 다른 정부가 수립되었고, 1950년에는 한국전쟁이 발발하여 큰 피해를 입었습니다. 전쟁 후 남한은 빠른 경제 발전을 이루며 오늘날에 이르렀습니다.\n",
      "\n",
      "한국의 역사는 오랜 시간 동안 수많은 시련과 도전을 겪으며 형성된 깊은 유산을 지니고 있습니다. 오늘날 한국은 그 역사적 배경 위에서 세계적으로 중요한 역할을 하고 있으며, 과거의 역사가 현재와 미래에 어떻게 영향을 미치는지를 이해하는 것은 매우 중요합니다.\n",
      "\n",
      "휴먼(주) 대표이사는? 홍길동 대표입니다.\n",
      "휴먼 주요 솔루션은? ERP(전사적자원관리) 시스템, 업무포털, PMS 등이 있습니다.\n",
      "휴먼 직원은 몇 명인가요? 40명 이내입니다.\n",
      "휴먼 영업대표는? 강건한 과장입니다.\n",
      "휴먼 경영지원팀장은? 김샛빛 차장입니다.\n",
      "휴먼 미션은? 기술과 비즈니스를 융합하여 새로운 미래를 만드는 기업\n",
      "휴먼 비전은? 정보기술의 가치 창출로 고객에겐 가치를 직원에겐 행복을 주는 기업\n",
      "휴먼 인재상은? 성실, 선의(선한 의도), 능력, 실적\n",
      "휴먼 주소는? 수원시 팔달구 중부대로 100 \n",
      "휴먼 분위기는? 일할 때는 조용합니다. 존중하는 분위기.\n",
      "휴먼 단체 해외여행 간 곳은? 중국 상하이, 항주, 심천, 홍콩, 마카오\n",
      "조용한 이사 성격은? 말이 없고 조용합니다. 그래서 재미가 없지만 책임감이 있음.\n",
      "휴먼 2024년 영업 매출 목표액은? 매출액 목표는 30억입니다.\n",
      "휴먼에서 가까운 역은? 수원역\n",
      "휴먼 창립년도는? 2010년 1월 1일\n",
      "휴먼 홈페이지 주소는? http://www.human.co.kr\n",
      "휴먼 주요사업은? SI(시스템통합), SM(시스템유지보수), 솔루션개발, ISP(정보화전략계획), ISMP(정보화전략마스터플랜), AI(인공지능), 시각화\n",
      "성진실 차장은 대전에서 살고 있고, 2024년 프로젝트로 인하여 서울로 출퇴근 하고 있음. 그래서 고생이 많음. 10년 이상 장기근속을 하고 있고 성실함. 책임감이 있음\n",
      "서울에서 대전까지의 거리는 151km. 성진실 차장은 2024년 10월 2일 기준으로 아직 결혼을 안했음.\n",
      "\n",
      "패밀리데이는 매주 금요일 30분 일찍 퇴근하는 휴먼 복리후생중의 하나임\n",
      "휴먼 인증 또는 인증서는? 기업부설연구소, 이노비즈, 메인비즈, 특허(병해충예찰 시스템) 등을 보유하고 있음\n",
      "김미리 사원은 맡은 일에 성실하고 최선을 다합니다.\n",
      "김미리 사원은 현재 테스트연구원 ERP 유지보수를 담당하고 있습니다.\n",
      "2024년 10월 22일은 날씨가 비가 오고 있어.\n",
      "휴먼 점심시간은? 원래 12시부터 13시까지 이지만, 식사 시간 혼잡을 피하기 위해 11:30부터 12:30까지 실시하고 있음\n",
      "성진실 차장은 기타를 잘치고 노래고 잘함\n",
      "2024년 11월 14일은 수능일입니다. 날씨는 흐렸습니다.\n",
      "2025년 1월 16일은 목요일입니다.\n",
      "\n",
      "    사용자 질문: 중심내용은 무엇입니까?\n",
      "\n",
      "    지침:\n",
      "    1. 문서에서 찾은 정보가 있다면 그 정보를 바탕으로 답변해주세요.\n",
      "    2. 문서에서 관련 정보를 찾지 못했다면 \"죄송합니다만, 해당 질문에 대한 정보를 문서에서 찾을 수 없습니다.\"라고 답변한 후, 일반적인 대화를 이어갈 수 있습니다.\n",
      "    3. 모든 답변은 친절하고 전문적으로 제공해주세요.\n",
      "\n",
      "    답변:\n",
      "    \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: 중심내용은 무엇입니까?\n",
      "Assistant: 제공된 문서는 휴먼(주)에 대한 정보와 한국 역사 개요를 담고 있습니다. 따라서 중심 내용은 **휴먼(주) 회사 소개 및 한국 역사** 입니다. \n",
      "\n",
      "\n",
      "\n",
      "Follow Up Input: 휴먼(주)의 주소는 어디인가요?\n",
      "Standalone question:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 3 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    당신은 휴먼(주) 정보를 제공하는 AI 어시스턴트입니다. 모든 답변은 한국어로 답변해 주세요.\n",
      "\n",
      "    아래는 이전 대화 내용입니다:\n",
      "    \n",
      "Human: 중심내용은 무엇입니까?\n",
      "Assistant: 제공된 문서는 휴먼(주)에 대한 정보와 한국 역사 개요를 담고 있습니다. 따라서 중심 내용은 **휴먼(주) 회사 소개 및 한국 역사** 입니다. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    관련 문서 내용:\n",
      "    휴먼(주) 대표이사는? 홍길동 대표입니다.\n",
      "휴먼 주요 솔루션은? ERP(전사적자원관리) 시스템, 업무포털, PMS 등이 있습니다.\n",
      "휴먼 직원은 몇 명인가요? 40명 이내입니다.\n",
      "휴먼 영업대표는? 강건한 과장입니다.\n",
      "휴먼 경영지원팀장은? 김샛빛 차장입니다.\n",
      "휴먼 미션은? 기술과 비즈니스를 융합하여 새로운 미래를 만드는 기업\n",
      "휴먼 비전은? 정보기술의 가치 창출로 고객에겐 가치를 직원에겐 행복을 주는 기업\n",
      "휴먼 인재상은? 성실, 선의(선한 의도), 능력, 실적\n",
      "휴먼 주소는? 수원시 팔달구 중부대로 100 \n",
      "휴먼 분위기는? 일할 때는 조용합니다. 존중하는 분위기.\n",
      "휴먼 단체 해외여행 간 곳은? 중국 상하이, 항주, 심천, 홍콩, 마카오\n",
      "조용한 이사 성격은? 말이 없고 조용합니다. 그래서 재미가 없지만 책임감이 있음.\n",
      "휴먼 2024년 영업 매출 목표액은? 매출액 목표는 30억입니다.\n",
      "휴먼에서 가까운 역은? 수원역\n",
      "휴먼 창립년도는? 2010년 1월 1일\n",
      "휴먼 홈페이지 주소는? http://www.human.co.kr\n",
      "휴먼 주요사업은? SI(시스템통합), SM(시스템유지보수), 솔루션개발, ISP(정보화전략계획), ISMP(정보화전략마스터플랜), AI(인공지능), 시각화\n",
      "성진실 차장은 대전에서 살고 있고, 2024년 프로젝트로 인하여 서울로 출퇴근 하고 있음. 그래서 고생이 많음. 10년 이상 장기근속을 하고 있고 성실함. 책임감이 있음\n",
      "서울에서 대전까지의 거리는 151km. 성진실 차장은 2024년 10월 2일 기준으로 아직 결혼을 안했음.\n",
      "\n",
      "패밀리데이는 매주 금요일 30분 일찍 퇴근하는 휴먼 복리후생중의 하나임\n",
      "휴먼 인증 또는 인증서는? 기업부설연구소, 이노비즈, 메인비즈, 특허(병해충예찰 시스템) 등을 보유하고 있음\n",
      "김미리 사원은 맡은 일에 성실하고 최선을 다합니다.\n",
      "김미리 사원은 현재 테스트연구원 ERP 유지보수를 담당하고 있습니다.\n",
      "2024년 10월 22일은 날씨가 비가 오고 있어.\n",
      "휴먼 점심시간은? 원래 12시부터 13시까지 이지만, 식사 시간 혼잡을 피하기 위해 11:30부터 12:30까지 실시하고 있음\n",
      "성진실 차장은 기타를 잘치고 노래고 잘함\n",
      "2024년 11월 14일은 수능일입니다. 날씨는 흐렸습니다.\n",
      "2025년 1월 16일은 목요일입니다.\n",
      "\n",
      "한국의 역사는 수천 년에 걸쳐 이어져 온 긴 여정 속에서 다양한 문화와 전통이 형성되고 발전해 왔습니다. 고조선에서 시작해 삼국 시대의 경쟁, 그리고 통일 신라와 고려를 거쳐 조선까지, 한반도는 많은 변화를 겪었습니다.\n",
      "\n",
      "고조선은 기원전 2333년 단군왕검에 의해 세워졌다고 전해집니다. 이는 한국 역사상 최초의 국가로, 한민족의 시원이라 할 수 있습니다. 이후 기원전 1세기경에는 한반도와 만주 일대에서 여러 소국이 성장하며 삼한 시대로 접어듭니다.\n",
      "\n",
      "4세기경, 고구려, 백제, 신라의 삼국이 한반도의 주요 세력으로 부상했습니다. 이 시기는 삼국이 각각 문화와 기술, 무력을 발전시키며 경쟁적으로 성장한 시기로, 한국 역사에서 중요한 전환점을 마련했습니다. 특히 고구려는 북방의 강대국으로 성장하여 중국과도 여러 차례 전쟁을 벌였습니다.\n",
      "\n",
      "7세기 말, 신라는 당나라와 연합하여 백제와 고구려를 차례로 정복하고, 한반도 최초의 통일 국가인 통일 신라를 건립합니다. 이 시기에 신라는 불교를 국교로 채택하며 문화와 예술이 크게 발전했습니다.\n",
      "\n",
      "그러나 10세기에 이르러 신라는 내부의 분열과 외부의 압력으로 쇠퇴하고, 이를 대체하여 고려가 성립됩니다. 고려 시대에는 과거제도의 도입과 더불어 청자 등 고려 고유의 문화가 꽃피었습니다.\n",
      "\n",
      "조선은 1392년 이성계에 의해 건국되어, 1910년까지 이어졌습니다. 조선 초기에는 세종대왕이 한글을 창제하여 백성들의 문해율을 높이는 등 문화적, 과학적 성취가 이루어졌습니다. 그러나 조선 후기에는 내부적으로 실학의 발전과 함께 사회적 변화가 모색되었으나, 외부로부터의 압력은 점차 커져만 갔습니다.\n",
      "\n",
      "19세기 말부터 20세기 초에 걸쳐 한국은 제국주의 열강의 침략을 받으며 많은 시련을 겪었습니다. 1910년, 한국은 일본에 의해 강제로 병합되어 35년간의 식민 지배를 받게 됩니다. 이 기간 동안 한국인들은 독립을 위한 다양한 운동을 전개했으며, 이는 1945년 일본의 패망으로 이어지는 독립으로 결실을 맺었습니다.\n",
      "\n",
      "해방 후 한반도는 남북으로 분단되어 각각 다른 정부가 수립되었고, 1950년에는 한국전쟁이 발발하여 큰 피해를 입었습니다. 전쟁 후 남한은 빠른 경제 발전을 이루며 오늘날에 이르렀습니다.\n",
      "\n",
      "한국의 역사는 오랜 시간 동안 수많은 시련과 도전을 겪으며 형성된 깊은 유산을 지니고 있습니다. 오늘날 한국은 그 역사적 배경 위에서 세계적으로 중요한 역할을 하고 있으며, 과거의 역사가 현재와 미래에 어떻게 영향을 미치는지를 이해하는 것은 매우 중요합니다.\n",
      "\n",
      "    사용자 질문: 휴먼(주)의 주소는 어디입니까?  \n",
      "\n",
      "\n",
      "    지침:\n",
      "    1. 문서에서 찾은 정보가 있다면 그 정보를 바탕으로 답변해주세요.\n",
      "    2. 문서에서 관련 정보를 찾지 못했다면 \"죄송합니다만, 해당 질문에 대한 정보를 문서에서 찾을 수 없습니다.\"라고 답변한 후, 일반적인 대화를 이어갈 수 있습니다.\n",
      "    3. 모든 답변은 친절하고 전문적으로 제공해주세요.\n",
      "\n",
      "    답변:\n",
      "    \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Garbage Collection 실행\n",
    "gc.collect()  # 메모리 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84b8839",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
